[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
<<<<<<< HEAD
    "objectID": "posts/Linear regression/lr.html",
    "href": "posts/Linear regression/lr.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nLinear regression is a powerful statistical method that allows us to examine the relationship between two (simple linear regression) or more (multiple linear regression) variables. A key feature of linear regression is its simplicity and interpretability. It’s used in various fields, including machine learning, most medical fields, and social sciences.\nLinear regression models the relationship between two variables by fitting a linear equation to observed data. The steps to perform multiple linear regression are almost identical to those of simple linear regression.\n\nTypes of regression:\na. Simple linear regression: In this case, we use only one input variable\nb. Multiple linear regression: In this case, We use multiple input variable\n\n\nThe Dataset\nI will update about it soon.\n\n\nAssumptions:\nThere are several assumption for the linear regression model that I am using in this blog. They are explained below, lets take a look.\nLinearity: The relationship between the independent and dependent variables is linear. This assumption can be checked by plotting the variables and examining the data scatter.\nIndependence: The residuals (the differences between the observed and predicted values) are independent. In other words, there is not a specific pattern in the residuals.\nHomoscedasticity: The variance of the residuals is constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all predicted values.\nNormality: The residuals are normally distributed. If this assumption is violated, then the confidence intervals may not be accurate.\nAbsence of multicollinearity: In the case of multiple linear regression, the independent variables should not be too highly correlated with each other.\nLoad necessary libraries:\nSet the random seed for reproducibility:\n\n# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(reshape2)\n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nLoad and clean the data:\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n# Handle missing values (if any)\n# data &lt;- na.omit(data)  # drops rows with missing values\n# data &lt;- data[complete.cases(data), ]  # drops rows with missing values\n# data$column &lt;- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n\nSince, we don’t have any null or missing value in this data as indicated above. So, no further handling of missing data is needed.\nVisualize the data: for better understanding of the data before proceeding further\n\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets take a look for the correlation between these two features: Our data contains one column “Species” which is non-numeric. Thus, we will firstly subset the data that doesnot contains “Species” column.\n\n# Subset the data to include only numeric columns\ndata_numeric &lt;- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n\n\nCreate the heatmap for better visualization:\n\n# Melt the correlation matrix into a long format\ndata_melt &lt;- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n\n\n\n\nFrom the heatmap created using the above code, we can see there is no strong correlation between different variables. However, this part is made to visualize the data only.\nBuilding the model: From the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\n\n# Create a violin plot for 'age'\nggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellow\") +\n  labs(title=\"Distribution of Charges by Sex\", x=\"Sex\", y=\"Charges\")\n\n\n\n# Create a violin plot for 'charges' vs 'sex'\np1 &lt;- ggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellowgreen\") +\n  labs(title=\"Violin plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a violin plot for 'charges' vs 'smoker'\np2 &lt;- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_violin(fill=\"goldenrod1\") +\n  labs(title=\"Violin plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n\n\nprint(p2)\n\n\n\n\nAccording to the left plot, the average insurance cost for men and women is roughly the same at $5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about $5,000. The minimum insurance premium for smokers is $5,000.\n\nNeed to check again on these\n\n# Split the data into training and test sets\nset.seed(0)\nsampleSplit &lt;- sample.split(Y=data$bmi, SplitRatio=0.7)\ntrainSet &lt;- subset(x=data, sampleSplit==TRUE)\ntestSet &lt;- subset(x=data, sampleSplit==FALSE)\n\n\n# Fit the model\nmodel &lt;- lm(formula=bmi ~ ., data=trainSet)\n\nBy using a linear combination of all other attributes, we hope to predict the bmi attribute. In this case, we don’t have to worry about the categorical attributes. R automatically manages the categorical attributes.\nLets see the summary of model:\n\n# Split the data into training and test sets\nsummary(model)\n\n\nCall:\nlm(formula = bmi ~ ., data = trainSet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.9234  -3.7146   0.0124   3.4208  20.9629 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.766e+01  6.883e-01  40.181  &lt; 2e-16 ***\nage             -2.573e-02  1.521e-02  -1.692   0.0910 .  \nsexmale          7.230e-01  3.679e-01   1.965   0.0497 *  \nchildren        -2.269e-02  1.504e-01  -0.151   0.8801    \nsmokeryes       -6.980e+00  7.894e-01  -8.841  &lt; 2e-16 ***\nregionnorthwest -2.051e-01  5.294e-01  -0.387   0.6986    \nregionsoutheast  3.923e+00  5.169e-01   7.589 7.58e-14 ***\nregionsouthwest  1.379e+00  5.307e-01   2.599   0.0095 ** \ncharges          2.803e-04  2.771e-05  10.119  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.73 on 967 degrees of freedom\nMultiple R-squared:  0.181, Adjusted R-squared:  0.1742 \nF-statistic: 26.71 on 8 and 967 DF,  p-value: &lt; 2.2e-16\n\n\nThe P-values that are shown in the Pr(&gt;|t|) column are the most intriguing aspect of this situation. The probability significance column Pr(&gt;|t|) in the above table is a crucial factor to take into account because it shows how important probability is for prediction. Here, we took a significance level of 5%  for the calculation.\n\nmodelResiduals &lt;- as.data.frame(residuals(model))\nggplot(modelResiduals, aes(residuals(model)))+\n  geom_histogram(fill = 'mediumorchid1', color = 'black')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe distribution of the residual data is approximately normally distributed.\nMake the prediction:\n\npreds &lt;- predict(model, testSet)\n\nNow, see the further details by creating a table for for actual and prediction values from the above created model.\n\nmodelEval &lt;- cbind(testSet$bmi, preds)\ncolnames(modelEval) &lt;- c('Acual', 'Predicted')\nmodelEval &lt;- as.data.frame(modelEval)\nhead(modelEval)\n\n    Acual Predicted\n4  22.705  33.49025\n14 39.820  33.24960\n16 24.600  29.76384\n23 34.100  32.15970\n24 31.920  30.35069\n27 23.085  30.08882\n\n\n\n\n\nNow lets evaluate the prediction:\nModel evaluation: After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse).\nLets learn little bit about rmse: The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\nDuring model evaluation, RMSE serves as a measure to understand the model’s performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\n\n# Calculate error metrics\nmse &lt;- mean((modelEval$Acual - modelEval$Predicted)^2)\nrmse &lt;- sqrt(mse)\nprint(rmse)\n\n[1] 4.962662"
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html",
    "href": "posts/Classification_Crop suitability/crop.html",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html#introduction",
    "href": "posts/Classification_Crop suitability/crop.html#introduction",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html#random-forest-rf",
    "href": "posts/Classification_Crop suitability/crop.html#random-forest-rf",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Random Forest (RF)",
    "text": "Random Forest (RF)\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees."
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html#support-vector-machines-svm",
    "href": "posts/Classification_Crop suitability/crop.html#support-vector-machines-svm",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It’s effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html#naive-bayes-nb",
    "href": "posts/Classification_Crop suitability/crop.html#naive-bayes-nb",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Naive Bayes (NB)",
    "text": "Naive Bayes (NB)\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area’s average temperature, humidity, pH level, rainfall, and NPK values."
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html#the-data",
    "href": "posts/Classification_Crop suitability/crop.html#the-data",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "The Data",
    "text": "The Data\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type.\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R’s head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R’s is.na() function."
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html#data-visualization",
    "href": "posts/Classification_Crop suitability/crop.html#data-visualization",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo get a better understanding of the data, I visualized it using R’s ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns."
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html#data-processing",
    "href": "posts/Classification_Crop suitability/crop.html#data-processing",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Processing",
    "text": "Data Processing\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label."
  },
  {
    "objectID": "posts/Classification_Crop suitability/crop.html#model-selection",
    "href": "posts/Classification_Crop suitability/crop.html#model-selection",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Model Selection",
    "text": "Model Selection\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\nLoad necessary libraries:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pheatmap)\n\nLoad the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/Crop_recommendation.csv\")\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\nset.seed(123)\n\nSplit the data into training and testing sets:\n\n# See the head and tail of data\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n\nCreate a data frame to store test data and predicted labels:\n\n# See the distribution of data \nsummary(data)\n\n       N                P                K           temperature    \n Min.   :  0.00   Min.   :  5.00   Min.   :  5.00   Min.   : 8.826  \n 1st Qu.: 21.00   1st Qu.: 28.00   1st Qu.: 20.00   1st Qu.:22.769  \n Median : 37.00   Median : 51.00   Median : 32.00   Median :25.599  \n Mean   : 50.55   Mean   : 53.36   Mean   : 48.15   Mean   :25.616  \n 3rd Qu.: 84.25   3rd Qu.: 68.00   3rd Qu.: 49.00   3rd Qu.:28.562  \n Max.   :140.00   Max.   :145.00   Max.   :205.00   Max.   :43.675  \n    humidity           ph           rainfall         label          \n Min.   :14.26   Min.   :3.505   Min.   : 20.21   Length:2200       \n 1st Qu.:60.26   1st Qu.:5.972   1st Qu.: 64.55   Class :character  \n Median :80.47   Median :6.425   Median : 94.87   Mode  :character  \n Mean   :71.48   Mean   :6.469   Mean   :103.46                     \n 3rd Qu.:89.95   3rd Qu.:6.924   3rd Qu.:124.27                     \n Max.   :99.98   Max.   :9.935   Max.   :298.56                     \n\n\nSee the relationship between different parameters such as Ph vs label or precipitation vs crop:\n\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n\n\n\n\n\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n\n\n\n\nData processing: separate feature and target variable:\n\nfeatures &lt;- data[,1:7]\ntarget &lt;- data$label\n\nSplit the data into training and testing sets:\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(target, p=0.8, list=FALSE)\ntrainData &lt;- features[trainIndex, ]\ntrainLabels &lt;- target[trainIndex]\ntestData &lt;- features[-trainIndex, ]\ntestLabels &lt;- target[-trainIndex]\n\nDefine training control:\n\ntrain_control &lt;- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm &lt;- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb &lt;- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels &lt;- as.factor(trainLabels)\ntestLabels &lt;- as.factor(testLabels)\n\nMake predictions on the test data:\nUsing Random Forest:\n\npred_rf &lt;- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf &lt;- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric &lt;- as.matrix(cm_rf)\ncm_rf_numeric[] &lt;- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n\n\n\n\nFor SVM\n\npred_svm &lt;- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm &lt;- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\nFor NB\n\npred_nb &lt;- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb &lt;- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric &lt;- as.matrix(cm_nb)\ncm_nb_numeric[] &lt;- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n\n\nCalculate and print misclassification rates:\n\nmis_rf &lt;- 1 - cm_rf$overall['Accuracy']\nmis_svm &lt;- 1 - cm_svm$overall['Accuracy']\nmis_nb &lt;- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification &lt;- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n          Model Misclassification_Rate\n1 Random Forest            0.004545455\n2           SVM            0.018181818\n3   Naive Bayes            0.004545455\n\n\n\n# Summarize the results\nresults &lt;- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: RF, SVM, NB \nNumber of resamples: 10 \n\nAccuracy \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9886364 0.9943182 0.9943182 0.9954545 0.9985795    1    0\nSVM 0.9659091 0.9730114 0.9829545 0.9829545 0.9928977    1    0\nNB  0.9772727 0.9943182 0.9943182 0.9943182 1.0000000    1    0\n\nKappa \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9880952 0.9940476 0.9940476 0.9952381 0.9985119    1    0\nSVM 0.9642857 0.9717262 0.9821429 0.9821429 0.9925595    1    0\nNB  0.9761905 0.9940476 0.9940476 0.9940476 1.0000000    1    0\n\n\n\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n\n\n\n\nMaking Predictions:\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n# Train the model\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData &lt;- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction &lt;- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\n\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\nConclusion\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant."
=======
    "objectID": "posts/Linear regression/index.html",
    "href": "posts/Linear regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nLinear regression is a powerful statistical method that allows us to examine the relationship between two (simple linear regression) or more (multiple linear regression) variables. A key feature of linear regression is its simplicity and interpretability. It’s used in various fields, including machine learning, most medical fields, and social sciences.\nLinear regression models the relationship between two variables by fitting a linear equation to observed data. The steps to perform multiple linear regression are almost identical to those of simple linear regression.\n\nTypes of regression:\na. Simple linear regression: In this case, we use only one input variable\nb. Multiple linear regression: In this case, We use multiple input variable\n\n\nThe Dataset\nI will update about it soon.\n\n\nAssumptions:\nThere are several assumption for the linear regression model that I am using in this blog. They are explained below, lets take a look.\nLinearity: The relationship between the independent and dependent variables is linear. This assumption can be checked by plotting the variables and examining the data scatter.\nIndependence: The residuals (the differences between the observed and predicted values) are independent. In other words, there is not a specific pattern in the residuals.\nHomoscedasticity: The variance of the residuals is constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all predicted values.\nNormality: The residuals are normally distributed. If this assumption is violated, then the confidence intervals may not be accurate.\nAbsence of multicollinearity: In the case of multiple linear regression, the independent variables should not be too highly correlated with each other.\nLoad necessary libraries:\nSet the random seed for reproducibility:\n\n# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(reshape2)\n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetri.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nLoad and clean the data:\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n# Handle missing values (if any)\n# data &lt;- na.omit(data)  # drops rows with missing values\n# data &lt;- data[complete.cases(data), ]  # drops rows with missing values\n# data$column &lt;- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n\nSince, we don’t have any null or missing value in this data as indicated above. So, no further handling of missing data is needed.\nVisualize the data: for better understanding of the data before proceeding further\n\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets take a look for the correlation between these two features: Our data contains one column “Species” which is non-numeric. Thus, we will firstly subset the data that doesnot contains “Species” column.\n\n# Subset the data to include only numeric columns\ndata_numeric &lt;- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n\n\nCreate the heatmap for better visualization:\n\n# Melt the correlation matrix into a long format\ndata_melt &lt;- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n\n\n\n\nFrom the heatmap created using the above code, we can see there is no strong correlation between different variables. However, this part is made to visualize the data only.\nBuilding the model: From the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\n\n# Create a violin plot for 'age'\nggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellow\") +\n  labs(title=\"Distribution of Charges by Sex\", x=\"Sex\", y=\"Charges\")\n\n\n\n# Create a violin plot for 'charges' vs 'sex'\np1 &lt;- ggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellowgreen\") +\n  labs(title=\"Violin plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a violin plot for 'charges' vs 'smoker'\np2 &lt;- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_violin(fill=\"goldenrod1\") +\n  labs(title=\"Violin plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n\n\nprint(p2)\n\n\n\n\nAccording to the left plot, the average insurance cost for men and women is roughly the same at $5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about $5,000. The minimum insurance premium for smokers is $5,000.\n\nNeed to check again on these\n\n# Split the data into training and test sets\nset.seed(0)\nsampleSplit &lt;- sample.split(Y=data$bmi, SplitRatio=0.7)\ntrainSet &lt;- subset(x=data, sampleSplit==TRUE)\ntestSet &lt;- subset(x=data, sampleSplit==FALSE)\n\n\n# Fit the model\nmodel &lt;- lm(formula=bmi ~ ., data=trainSet)\n\nBy using a linear combination of all other attributes, we hope to predict the bmi attribute. In this case, we don’t have to worry about the categorical attributes. R automatically manages the categorical attributes.\nLets see the summary of model:\n\n# Split the data into training and test sets\nsummary(model)\n\n\nCall:\nlm(formula = bmi ~ ., data = trainSet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.9234  -3.7146   0.0124   3.4208  20.9629 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.766e+01  6.883e-01  40.181  &lt; 2e-16 ***\nage             -2.573e-02  1.521e-02  -1.692   0.0910 .  \nsexmale          7.230e-01  3.679e-01   1.965   0.0497 *  \nchildren        -2.269e-02  1.504e-01  -0.151   0.8801    \nsmokeryes       -6.980e+00  7.894e-01  -8.841  &lt; 2e-16 ***\nregionnorthwest -2.051e-01  5.294e-01  -0.387   0.6986    \nregionsoutheast  3.923e+00  5.169e-01   7.589 7.58e-14 ***\nregionsouthwest  1.379e+00  5.307e-01   2.599   0.0095 ** \ncharges          2.803e-04  2.771e-05  10.119  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.73 on 967 degrees of freedom\nMultiple R-squared:  0.181, Adjusted R-squared:  0.1742 \nF-statistic: 26.71 on 8 and 967 DF,  p-value: &lt; 2.2e-16\n\n\nThe P-values that are shown in the Pr(&gt;|t|) column are the most intriguing aspect of this situation. The probability significance column Pr(&gt;|t|) in the above table is a crucial factor to take into account because it shows how important probability is for prediction. Here, we took a significance level of 5%  for the calculation.\n\nmodelResiduals &lt;- as.data.frame(residuals(model))\nggplot(modelResiduals, aes(residuals(model)))+\n  geom_histogram(fill = 'mediumorchid1', color = 'black')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe distribution of the residual data is approximately normally distributed.\nMake the prediction:\n\npreds &lt;- predict(model, testSet)\n\nNow, see the further details by creating a table for for actual and prediction values from the above created model.\n\nmodelEval &lt;- cbind(testSet$bmi, preds)\ncolnames(modelEval) &lt;- c('Acual', 'Predicted')\nmodelEval &lt;- as.data.frame(modelEval)\nhead(modelEval)\n\n    Acual Predicted\n4  22.705  33.49025\n14 39.820  33.24960\n16 24.600  29.76384\n23 34.100  32.15970\n24 31.920  30.35069\n27 23.085  30.08882\n\n\n\n\n\nNow lets evaluate the prediction:\nModel evaluation: After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse).\nLets learn little bit about rmse: The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\nDuring model evaluation, RMSE serves as a measure to understand the model’s performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\n\n# Calculate error metrics\nmse &lt;- mean((modelEval$Acual - modelEval$Predicted)^2)\nrmse &lt;- sqrt(mse)\nprint(rmse)\n\n[1] 4.962662"
  },
  {
    "objectID": "posts/decision tree using R/index.html",
    "href": "posts/decision tree using R/index.html",
    "title": "Decision tree and random forest with R",
    "section": "",
    "text": "Decision trees are a popular machine learning algorithm due to their simplicity, interpretability, and versatility. They can be used for both regression and classification problems. In this blog post, we will walk you through how to implement a decision tree in R.\nDecision Trees are a type of supervised Machine Learning algorithm used for both regression and classification problems. They are tree-based models that split the data into smaller subsets based on certain conditions. The final output is obtained by combining the results of multiple splits.\nThe Iris Dataset:\nThe Iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The dataset consists of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the lengths and the widths of the sepals and petals.\nLoad necessary libraries\n\nlibrary(rpart)\n\n\n\n\nnsample = nrow(iris)\n\nSet the random seed for reproducibility\n\nset.seed(0)\n\n\n\n\n\n# split data into training and test sets\ntrain.ix = sample(1 : nsample, round(nsample / 2))\ntest.ix = setdiff(1 : nsample, train.ix)\ntrain.df = iris[train.ix, ]; test.df = iris[test.ix, ]\n\n\n\n\n\nctmodel = rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length\n                + Petal.Width, method = \"class\", data = train.df)\nsummary(ctmodel) # detailed summary of splits\n\nCall:\nrpart(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + \n    Petal.Width, data = train.df, method = \"class\")\n  n= 75 \n\n         CP nsplit  rel error     xerror       xstd\n1 0.5869565      0 1.00000000 1.08695652 0.08874963\n2 0.3913043      1 0.41304348 0.41304348 0.08188085\n3 0.0100000      2 0.02173913 0.02173913 0.02159372\n\nVariable importance\n Petal.Width Petal.Length Sepal.Length  Sepal.Width \n          33           30           22           15 \n\nNode number 1: 75 observations,    complexity param=0.5869565\n  predicted class=setosa      expected loss=0.6133333  P(node) =1\n    class counts:    29    19    27\n   probabilities: 0.387 0.253 0.360 \n  left son=2 (29 obs) right son=3 (46 obs)\n  Primary splits:\n      Petal.Length &lt; 2.45 to the left,  improve=26.94899, (0 missing)\n      Petal.Width  &lt; 0.8  to the left,  improve=26.94899, (0 missing)\n      Sepal.Length &lt; 5.55 to the left,  improve=22.28490, (0 missing)\n      Sepal.Width  &lt; 3.35 to the right, improve=11.74540, (0 missing)\n  Surrogate splits:\n      Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.000, (0 split)\n      Sepal.Length &lt; 5.45 to the left,  agree=0.933, adj=0.828, (0 split)\n      Sepal.Width  &lt; 3.35 to the right, agree=0.840, adj=0.586, (0 split)\n\nNode number 2: 29 observations\n  predicted class=setosa      expected loss=0  P(node) =0.3866667\n    class counts:    29     0     0\n   probabilities: 1.000 0.000 0.000 \n\nNode number 3: 46 observations,    complexity param=0.3913043\n  predicted class=virginica   expected loss=0.4130435  P(node) =0.6133333\n    class counts:     0    19    27\n   probabilities: 0.000 0.413 0.587 \n  left son=6 (20 obs) right son=7 (26 obs)\n  Primary splits:\n      Petal.Width  &lt; 1.75 to the left,  improve=20.404350, (0 missing)\n      Petal.Length &lt; 4.75 to the left,  improve=16.904350, (0 missing)\n      Sepal.Length &lt; 6.15 to the left,  improve= 6.787779, (0 missing)\n      Sepal.Width  &lt; 2.75 to the left,  improve= 3.686701, (0 missing)\n  Surrogate splits:\n      Petal.Length &lt; 4.75 to the left,  agree=0.913, adj=0.80, (0 split)\n      Sepal.Length &lt; 6.15 to the left,  agree=0.761, adj=0.45, (0 split)\n      Sepal.Width  &lt; 2.75 to the left,  agree=0.696, adj=0.30, (0 split)\n\nNode number 6: 20 observations\n  predicted class=versicolor  expected loss=0.05  P(node) =0.2666667\n    class counts:     0    19     1\n   probabilities: 0.000 0.950 0.050 \n\nNode number 7: 26 observations\n  predicted class=virginica   expected loss=0  P(node) =0.3466667\n    class counts:     0     0    26\n   probabilities: 0.000 0.000 1.000 \n\n\nVisualize the decision tree:\n\n# Load necessary libraries\nlibrary(rpart.plot)\nlibrary(ggplot2)\n\n# Plot the decision tree\nrpart.plot(ctmodel, type = 4, extra = 2, tweak = 1.3, varlen = 0, nn = TRUE, fallen.leaves = TRUE,\n           box.palette = \"GnBu\", shadow.col = \"gray\", main = \"Classification Tree for Iris Data\")\n\n\n\n\nPerform classification for test set:\n\npred.id = predict(ctmodel, test.df, type = \"class\")\n\n\n# pred.mat = predict(ctmodel, test.df)\n# pred.numid = apply(pred.mat, 1, which.max)\n# mcr = mean(pred.numid != as.numeric(test.df$Species))\n# pred.id = cut(pred.numid, breaks = 0 : 3 + 0.5, labels = levels(test.id))\ntest.id = test.df$Species\nmcr = mean(pred.id != test.id)\ntable(test.id, pred.id)\n\n            pred.id\ntest.id      setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          4        19"
  },
  {
    "objectID": "posts/decision tree using R/index.html#introduction",
    "href": "posts/decision tree using R/index.html#introduction",
    "title": "Decision tree and random forest with R",
    "section": "",
    "text": "Decision trees are a popular machine learning algorithm due to their simplicity, interpretability, and versatility. They can be used for both regression and classification problems. In this blog post, we will walk you through how to implement a decision tree in R.\nDecision Trees are a type of supervised Machine Learning algorithm used for both regression and classification problems. They are tree-based models that split the data into smaller subsets based on certain conditions. The final output is obtained by combining the results of multiple splits.\nThe Iris Dataset:\nThe Iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The dataset consists of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the lengths and the widths of the sepals and petals.\nLoad necessary libraries\n\nlibrary(rpart)\n\n\n\n\nnsample = nrow(iris)\n\nSet the random seed for reproducibility\n\nset.seed(0)\n\n\n\n\n\n# split data into training and test sets\ntrain.ix = sample(1 : nsample, round(nsample / 2))\ntest.ix = setdiff(1 : nsample, train.ix)\ntrain.df = iris[train.ix, ]; test.df = iris[test.ix, ]\n\n\n\n\n\nctmodel = rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length\n                + Petal.Width, method = \"class\", data = train.df)\nsummary(ctmodel) # detailed summary of splits\n\nCall:\nrpart(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + \n    Petal.Width, data = train.df, method = \"class\")\n  n= 75 \n\n         CP nsplit  rel error     xerror       xstd\n1 0.5869565      0 1.00000000 1.08695652 0.08874963\n2 0.3913043      1 0.41304348 0.41304348 0.08188085\n3 0.0100000      2 0.02173913 0.02173913 0.02159372\n\nVariable importance\n Petal.Width Petal.Length Sepal.Length  Sepal.Width \n          33           30           22           15 \n\nNode number 1: 75 observations,    complexity param=0.5869565\n  predicted class=setosa      expected loss=0.6133333  P(node) =1\n    class counts:    29    19    27\n   probabilities: 0.387 0.253 0.360 \n  left son=2 (29 obs) right son=3 (46 obs)\n  Primary splits:\n      Petal.Length &lt; 2.45 to the left,  improve=26.94899, (0 missing)\n      Petal.Width  &lt; 0.8  to the left,  improve=26.94899, (0 missing)\n      Sepal.Length &lt; 5.55 to the left,  improve=22.28490, (0 missing)\n      Sepal.Width  &lt; 3.35 to the right, improve=11.74540, (0 missing)\n  Surrogate splits:\n      Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.000, (0 split)\n      Sepal.Length &lt; 5.45 to the left,  agree=0.933, adj=0.828, (0 split)\n      Sepal.Width  &lt; 3.35 to the right, agree=0.840, adj=0.586, (0 split)\n\nNode number 2: 29 observations\n  predicted class=setosa      expected loss=0  P(node) =0.3866667\n    class counts:    29     0     0\n   probabilities: 1.000 0.000 0.000 \n\nNode number 3: 46 observations,    complexity param=0.3913043\n  predicted class=virginica   expected loss=0.4130435  P(node) =0.6133333\n    class counts:     0    19    27\n   probabilities: 0.000 0.413 0.587 \n  left son=6 (20 obs) right son=7 (26 obs)\n  Primary splits:\n      Petal.Width  &lt; 1.75 to the left,  improve=20.404350, (0 missing)\n      Petal.Length &lt; 4.75 to the left,  improve=16.904350, (0 missing)\n      Sepal.Length &lt; 6.15 to the left,  improve= 6.787779, (0 missing)\n      Sepal.Width  &lt; 2.75 to the left,  improve= 3.686701, (0 missing)\n  Surrogate splits:\n      Petal.Length &lt; 4.75 to the left,  agree=0.913, adj=0.80, (0 split)\n      Sepal.Length &lt; 6.15 to the left,  agree=0.761, adj=0.45, (0 split)\n      Sepal.Width  &lt; 2.75 to the left,  agree=0.696, adj=0.30, (0 split)\n\nNode number 6: 20 observations\n  predicted class=versicolor  expected loss=0.05  P(node) =0.2666667\n    class counts:     0    19     1\n   probabilities: 0.000 0.950 0.050 \n\nNode number 7: 26 observations\n  predicted class=virginica   expected loss=0  P(node) =0.3466667\n    class counts:     0     0    26\n   probabilities: 0.000 0.000 1.000 \n\n\nVisualize the decision tree:\n\n# Load necessary libraries\nlibrary(rpart.plot)\nlibrary(ggplot2)\n\n# Plot the decision tree\nrpart.plot(ctmodel, type = 4, extra = 2, tweak = 1.3, varlen = 0, nn = TRUE, fallen.leaves = TRUE,\n           box.palette = \"GnBu\", shadow.col = \"gray\", main = \"Classification Tree for Iris Data\")\n\n\n\n\nPerform classification for test set:\n\npred.id = predict(ctmodel, test.df, type = \"class\")\n\n\n# pred.mat = predict(ctmodel, test.df)\n# pred.numid = apply(pred.mat, 1, which.max)\n# mcr = mean(pred.numid != as.numeric(test.df$Species))\n# pred.id = cut(pred.numid, breaks = 0 : 3 + 0.5, labels = levels(test.id))\ntest.id = test.df$Species\nmcr = mean(pred.id != test.id)\ntable(test.id, pred.id)\n\n            pred.id\ntest.id      setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          4        19"
>>>>>>> 59d757e7d394f5f4f5d9f096b3d37f9fbb9321dd
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
<<<<<<< HEAD
    "text": "This is a post with executable code. edit this code:"
  },
  {
    "objectID": "posts/Probability/pr.html",
    "href": "posts/Probability/pr.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Machine learning, at its core, is deeply rooted in statistics and mathematics. One of the fundamental concepts that underpin these fields is probability theory. In this blog post, we will delve into the world of probability theory and random variables, and explore their significance in machine learning.\n\n\n\nProbability theory is a branch of mathematics concerned with the analysis of random phenomena. It provides a mathematical framework for quantifying uncertainty, which is a common feature in real-world data and scenarios.\nIn machine learning, probability theory is used to make predictions. For instance, in a classification problem, a model might predict the probability of a given data point belonging to a particular class.\n\n\n\nA random variable is a variable whose possible values are outcomes of a random phenomenon. There are two types of random variables: discrete and continuous.\n\nDiscrete Random Variables: These have a countable number of outcomes. Examples include the number of heads in a coin toss, or the number of defective items in a batch.\nContinuous Random Variables: These can take on any value in a given range. Examples include the height of a person, or the time it takes to run a mile.\n\nIn machine learning, random variables are often used to represent the inputs and outputs of a model.\n\n\n\nA probability distribution describes how a random variable is distributed; it tells us which outcomes are likely, and which are not. In machine learning, understanding the underlying probability distribution of your data can be very helpful in selecting appropriate models.\nLoad necessary libraries\n\nlibrary(\"ggplot2\")"
  },
  {
    "objectID": "posts/Probability/pr.html#introduction",
    "href": "posts/Probability/pr.html#introduction",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Machine learning, at its core, is deeply rooted in statistics and mathematics. One of the fundamental concepts that underpin these fields is probability theory. In this blog post, we will delve into the world of probability theory and random variables, and explore their significance in machine learning."
  },
  {
    "objectID": "posts/Probability/pr.html#probability-theory",
    "href": "posts/Probability/pr.html#probability-theory",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Probability theory is a branch of mathematics concerned with the analysis of random phenomena. It provides a mathematical framework for quantifying uncertainty, which is a common feature in real-world data and scenarios.\nIn machine learning, probability theory is used to make predictions. For instance, in a classification problem, a model might predict the probability of a given data point belonging to a particular class."
  },
  {
    "objectID": "posts/Probability/pr.html#random-variables",
    "href": "posts/Probability/pr.html#random-variables",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "A random variable is a variable whose possible values are outcomes of a random phenomenon. There are two types of random variables: discrete and continuous.\n\nDiscrete Random Variables: These have a countable number of outcomes. Examples include the number of heads in a coin toss, or the number of defective items in a batch.\nContinuous Random Variables: These can take on any value in a given range. Examples include the height of a person, or the time it takes to run a mile.\n\nIn machine learning, random variables are often used to represent the inputs and outputs of a model."
  },
  {
    "objectID": "posts/Probability/pr.html#probability-distributions",
    "href": "posts/Probability/pr.html#probability-distributions",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "A probability distribution describes how a random variable is distributed; it tells us which outcomes are likely, and which are not. In machine learning, understanding the underlying probability distribution of your data can be very helpful in selecting appropriate models.\nLoad necessary libraries\n\nlibrary(\"ggplot2\")"
  },
  {
    "objectID": "posts/clustering/clustering.html",
    "href": "posts/clustering/clustering.html",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data's density.\n\n\n\nDBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least minPts within a radius of eps from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset.\n\n\n\nDBSCAN has several advantages over other clustering algorithms:\n\nNo need to specify the number of clusters: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\nAbility to find arbitrarily shaped clusters: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\nRobustness to noise: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria.\n\n\n\n\nDespite its advantages, DBSCAN also has some limitations:\n\nDifficulty handling varying densities: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single eps and minPts value may not be suitable for all clusters.\nSensitivity to parameter settings: The results of DBSCAN can be significantly affected by the settings of eps and minPts. Choosing appropriate values for these parameters can be challenging.\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\nLoad necessary libraries\n\nlibrary(fpc) \n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nThe dataset contains information about individuals such as their age, sex, BMI, number of children, smoking status, region, and charges.\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ..."
  },
  {
    "objectID": "posts/clustering/clustering.html#introduction",
    "href": "posts/clustering/clustering.html#introduction",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data's density."
  },
  {
    "objectID": "posts/clustering/clustering.html#how-dbscan-works",
    "href": "posts/clustering/clustering.html#how-dbscan-works",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "DBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least minPts within a radius of eps from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset."
  },
  {
    "objectID": "posts/clustering/clustering.html#advantages-of-dbscan",
    "href": "posts/clustering/clustering.html#advantages-of-dbscan",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "DBSCAN has several advantages over other clustering algorithms:\n\nNo need to specify the number of clusters: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\nAbility to find arbitrarily shaped clusters: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\nRobustness to noise: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria."
  },
  {
    "objectID": "posts/clustering/clustering.html#disadvantages-of-dbscan",
    "href": "posts/clustering/clustering.html#disadvantages-of-dbscan",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Despite its advantages, DBSCAN also has some limitations:\n\nDifficulty handling varying densities: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single eps and minPts value may not be suitable for all clusters.\nSensitivity to parameter settings: The results of DBSCAN can be significantly affected by the settings of eps and minPts. Choosing appropriate values for these parameters can be challenging.\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\nLoad necessary libraries\n\nlibrary(fpc) \n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nThe dataset contains information about individuals such as their age, sex, BMI, number of children, smoking status, region, and charges.\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ..."
  },
  {
    "objectID": "posts/clustering/clustering.html#remove-label-form-dataset",
    "href": "posts/clustering/clustering.html#remove-label-form-dataset",
    "title": "DNSCAN Clustering",
    "section": "Remove label form dataset",
    "text": "Remove label form dataset\n\nmed &lt;- data_num[-4] \n\nFitting DBSCAN clustering model:\n\nset.seed(0)  # Setting seed \nDbscan_cl &lt;- dbscan(med, eps = 0.45, MinPts = 5) \nDbscan_cl\n\ndbscan Pts=1338 MinPts=5 eps=0.45\n          0 1 2 3 4 5\nborder 1310 5 2 4 3 4\nseed      0 1 5 1 1 2\ntotal  1310 6 7 5 4 6\n\n\nIn this code, eps is the maximum distance between two samples for them to be considered as in the same neighborhood, and MinPts is the number of samples in a neighborhood for a point to be considered as a core point."
=======
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Introduction to Linear Regression using Machine Learning",
    "section": "",
    "text": "K-Nearest Neighbors (KNN) is a simple yet powerful classification algorithm. It classifies a data point based on how its neighbors are classified. In this blog post, we will use the Iris dataset to understand how KNN works using R.\n\n\n\nKNN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. It’s both a classification and regression method.\n\n\n\nThe Iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The dataset consists of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the lengths and the widths of the sepals and petals.\nLoad necessary libraries\n\nlibrary(class)\nlibrary(ggplot2)\n\nSet the random seed for reproducibility\n\nset.seed(0)\n\nSplit data into training and test sets\n\nnsample = nrow(iris)\ntrain.ix = sample(1:nsample, round(nsample / 2))\ntest.ix = setdiff(1:nsample, train.ix)\ntrain.X = iris[train.ix, 3:4]  # Only use Petal.Length, Petal.Width\ntrain.Y = iris[train.ix, 5]    # Factor data type\ntest.X = iris[test.ix, 3:4]\ntest.Y = iris[test.ix, 5]\n\nPerform classification for the test set\n\nk_value = 3\npred.Y = knn(train.X, test.X, train.Y, k = k_value)\nmcr = mean(test.Y != pred.Y)\n\nCreate a data frame to store test data and predicted labels\n\nresults_df &lt;- data.frame(\n  Petal.Length = test.X[, 1],\n  Petal.Width = test.X[, 2],\n  Actual_Class = test.Y,\n  Predicted_Class = pred.Y\n)\n\nPlot the test data points color-coded by their predicted class\n\nggplot(results_df, aes(x = Petal.Length, y = Petal.Width, color = factor(Actual_Class), shape = factor(Predicted_Class))) +\n  geom_point(size = 3) +\n  labs(x = \"Petal Length\", y = \"Petal Width\", title = paste(\"KNN Classification (k =\", k_value, \")\")) +\n  scale_color_discrete(name = \"Actual Class\") +\n  scale_shape_discrete(name = \"Predicted Class\") +\n  theme_minimal()\n\n\n\n\nPrint the confusion matrix\n\nconfusion_matrix &lt;- table(test.Y, pred.Y)\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(confusion_matrix)\n\n            pred.Y\ntest.Y       setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          3        20\n\n\nCalculate the misclassification rate\n\nprint(paste(\"Misclassification Rate:\", mcr))\n\n[1] \"Misclassification Rate: 0.0533333333333333\"\n\n\n\nprint(paste(\"Misclassification Rate:\", mcr))\n\n[1] \"Misclassification Rate: 0.0533333333333333\""
  },
  {
    "objectID": "posts/welcome/index.html#introduction",
    "href": "posts/welcome/index.html#introduction",
    "title": "Introduction to Linear Regression using Machine Learning",
    "section": "",
    "text": "K-Nearest Neighbors (KNN) is a simple yet powerful classification algorithm. It classifies a data point based on how its neighbors are classified. In this blog post, we will use the Iris dataset to understand how KNN works using R."
  },
  {
    "objectID": "posts/welcome/index.html#what-is-knn",
    "href": "posts/welcome/index.html#what-is-knn",
    "title": "Introduction to Linear Regression using Machine Learning",
    "section": "",
    "text": "KNN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. It’s both a classification and regression method."
  },
  {
    "objectID": "posts/welcome/index.html#the-iris-dataset",
    "href": "posts/welcome/index.html#the-iris-dataset",
    "title": "Introduction to Linear Regression using Machine Learning",
    "section": "",
    "text": "The Iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The dataset consists of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the lengths and the widths of the sepals and petals.\nLoad necessary libraries\n\nlibrary(class)\nlibrary(ggplot2)\n\nSet the random seed for reproducibility\n\nset.seed(0)\n\nSplit data into training and test sets\n\nnsample = nrow(iris)\ntrain.ix = sample(1:nsample, round(nsample / 2))\ntest.ix = setdiff(1:nsample, train.ix)\ntrain.X = iris[train.ix, 3:4]  # Only use Petal.Length, Petal.Width\ntrain.Y = iris[train.ix, 5]    # Factor data type\ntest.X = iris[test.ix, 3:4]\ntest.Y = iris[test.ix, 5]\n\nPerform classification for the test set\n\nk_value = 3\npred.Y = knn(train.X, test.X, train.Y, k = k_value)\nmcr = mean(test.Y != pred.Y)\n\nCreate a data frame to store test data and predicted labels\n\nresults_df &lt;- data.frame(\n  Petal.Length = test.X[, 1],\n  Petal.Width = test.X[, 2],\n  Actual_Class = test.Y,\n  Predicted_Class = pred.Y\n)\n\nPlot the test data points color-coded by their predicted class\n\nggplot(results_df, aes(x = Petal.Length, y = Petal.Width, color = factor(Actual_Class), shape = factor(Predicted_Class))) +\n  geom_point(size = 3) +\n  labs(x = \"Petal Length\", y = \"Petal Width\", title = paste(\"KNN Classification (k =\", k_value, \")\")) +\n  scale_color_discrete(name = \"Actual Class\") +\n  scale_shape_discrete(name = \"Predicted Class\") +\n  theme_minimal()\n\n\n\n\nPrint the confusion matrix\n\nconfusion_matrix &lt;- table(test.Y, pred.Y)\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(confusion_matrix)\n\n            pred.Y\ntest.Y       setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          3        20\n\n\nCalculate the misclassification rate\n\nprint(paste(\"Misclassification Rate:\", mcr))\n\n[1] \"Misclassification Rate: 0.0533333333333333\"\n\n\n\nprint(paste(\"Misclassification Rate:\", mcr))\n\n[1] \"Misclassification Rate: 0.0533333333333333\""
  },
  {
    "objectID": "posts/Crop suitability/index.html",
    "href": "posts/Crop suitability/index.html",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Crop suitability/index.html#introduction",
    "href": "posts/Crop suitability/index.html#introduction",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Crop suitability/index.html#random-forest-rf",
    "href": "posts/Crop suitability/index.html#random-forest-rf",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Random Forest (RF)",
    "text": "Random Forest (RF)\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees."
  },
  {
    "objectID": "posts/Crop suitability/index.html#support-vector-machines-svm",
    "href": "posts/Crop suitability/index.html#support-vector-machines-svm",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It’s effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
  },
  {
    "objectID": "posts/Crop suitability/index.html#naive-bayes-nb",
    "href": "posts/Crop suitability/index.html#naive-bayes-nb",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Naive Bayes (NB)",
    "text": "Naive Bayes (NB)\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area’s average temperature, humidity, pH level, rainfall, and NPK values."
  },
  {
    "objectID": "posts/Crop suitability/index.html#the-data",
    "href": "posts/Crop suitability/index.html#the-data",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "The Data",
    "text": "The Data\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type.\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R’s head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R’s is.na() function."
  },
  {
    "objectID": "posts/Crop suitability/index.html#data-visualization",
    "href": "posts/Crop suitability/index.html#data-visualization",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo get a better understanding of the data, I visualized it using R’s ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns."
  },
  {
    "objectID": "posts/Crop suitability/index.html#data-processing",
    "href": "posts/Crop suitability/index.html#data-processing",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Processing",
    "text": "Data Processing\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label."
  },
  {
    "objectID": "posts/Crop suitability/index.html#model-selection",
    "href": "posts/Crop suitability/index.html#model-selection",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Model Selection",
    "text": "Model Selection\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\nLoad necessary libraries:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pheatmap)\n\nLoad the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kchhetri.github.io/Crop_recommendation.csv\")\n\n\nset.seed(123)\n\nSplit the data into training and testing sets:\n\n# See the head and tail of data\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n\nCreate a data frame to store test data and predicted labels:\n\n# See the distribution of data \nsummary(data)\n\n       N                P                K           temperature    \n Min.   :  0.00   Min.   :  5.00   Min.   :  5.00   Min.   : 8.826  \n 1st Qu.: 21.00   1st Qu.: 28.00   1st Qu.: 20.00   1st Qu.:22.769  \n Median : 37.00   Median : 51.00   Median : 32.00   Median :25.599  \n Mean   : 50.55   Mean   : 53.36   Mean   : 48.15   Mean   :25.616  \n 3rd Qu.: 84.25   3rd Qu.: 68.00   3rd Qu.: 49.00   3rd Qu.:28.562  \n Max.   :140.00   Max.   :145.00   Max.   :205.00   Max.   :43.675  \n    humidity           ph           rainfall         label          \n Min.   :14.26   Min.   :3.505   Min.   : 20.21   Length:2200       \n 1st Qu.:60.26   1st Qu.:5.972   1st Qu.: 64.55   Class :character  \n Median :80.47   Median :6.425   Median : 94.87   Mode  :character  \n Mean   :71.48   Mean   :6.469   Mean   :103.46                     \n 3rd Qu.:89.95   3rd Qu.:6.924   3rd Qu.:124.27                     \n Max.   :99.98   Max.   :9.935   Max.   :298.56                     \n\n\nSee the relationship between different parameters such as Ph vs label or precipitation vs crop:\n\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n\n\n\n\n\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n\n\n\n\nData processing: separate feature and target variable:\n\nfeatures &lt;- data[,1:7]\ntarget &lt;- data$label\n\nSplit the data into training and testing sets:\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(target, p=0.8, list=FALSE)\ntrainData &lt;- features[trainIndex, ]\ntrainLabels &lt;- target[trainIndex]\ntestData &lt;- features[-trainIndex, ]\ntestLabels &lt;- target[-trainIndex]\n\nDefine training control:\n\ntrain_control &lt;- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm &lt;- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb &lt;- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels &lt;- as.factor(trainLabels)\ntestLabels &lt;- as.factor(testLabels)\n\nMake predictions on the test data:\nUsing Random Forest:\n\npred_rf &lt;- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf &lt;- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric &lt;- as.matrix(cm_rf)\ncm_rf_numeric[] &lt;- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n\n\n\n\nFor SVM\n\npred_svm &lt;- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm &lt;- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\nFor NB\n\npred_nb &lt;- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb &lt;- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric &lt;- as.matrix(cm_nb)\ncm_nb_numeric[] &lt;- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n\n\nCalculate and print misclassification rates:\n\nmis_rf &lt;- 1 - cm_rf$overall['Accuracy']\nmis_svm &lt;- 1 - cm_svm$overall['Accuracy']\nmis_nb &lt;- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification &lt;- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n          Model Misclassification_Rate\n1 Random Forest            0.004545455\n2           SVM            0.018181818\n3   Naive Bayes            0.004545455\n\n\n\n# Summarize the results\nresults &lt;- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: RF, SVM, NB \nNumber of resamples: 10 \n\nAccuracy \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9886364 0.9943182 0.9943182 0.9954545 0.9985795    1    0\nSVM 0.9659091 0.9730114 0.9829545 0.9829545 0.9928977    1    0\nNB  0.9772727 0.9943182 0.9943182 0.9943182 1.0000000    1    0\n\nKappa \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9880952 0.9940476 0.9940476 0.9952381 0.9985119    1    0\nSVM 0.9642857 0.9717262 0.9821429 0.9821429 0.9925595    1    0\nNB  0.9761905 0.9940476 0.9940476 0.9940476 1.0000000    1    0\n\n\n\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n\n\n\n\nMaking Predictions:\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n# Train the model\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData &lt;- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction &lt;- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\n\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\nConclusion\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant."
>>>>>>> 59d757e7d394f5f4f5d9f096b3d37f9fbb9321dd
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "kchhetri.github.io",
    "section": "",
<<<<<<< HEAD
    "text": "Predicting Crop Suitability with Machine Learning using R\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\nNews\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nDNSCAN Clustering\n\n\n\n\n\n\n\nNews\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\nNo matching items"
=======
    "text": "Predicting Crop Suitability with Machine Learning using R\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\nNews\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Probability/index.html",
    "href": "posts/Probability/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Load necessary libraries\n\nlibrary(class)\nlibrary(ggplot2)\n\nSet the random seed for reproducibility\n\nset.seed(0)\n\nSplit data into training and test sets\n\nnsample = nrow(iris)\ntrain.ix = sample(1:nsample, round(nsample / 2))\ntest.ix = setdiff(1:nsample, train.ix)\ntrain.X = iris[train.ix, 3:4]  # Only use Petal.Length, Petal.Width\ntrain.Y = iris[train.ix, 5]    # Factor data type\ntest.X = iris[test.ix, 3:4]\ntest.Y = iris[test.ix, 5]\n\nPerform classification for the test set\n\nk_value = 3\npred.Y = knn(train.X, test.X, train.Y, k = k_value)\nmcr = mean(test.Y != pred.Y)\n\nCreate a data frame to store test data and predicted labels\n\nresults_df &lt;- data.frame(\n  Petal.Length = test.X[, 1],\n  Petal.Width = test.X[, 2],\n  Actual_Class = test.Y,\n  Predicted_Class = pred.Y\n)\n\nPlot the test data points color-coded by their predicted class\n\nggplot(results_df, aes(x = Petal.Length, y = Petal.Width, color = factor(Actual_Class), shape = factor(Predicted_Class))) +\n  geom_point(size = 3) +\n  labs(x = \"Petal Length\", y = \"Petal Width\", title = paste(\"KNN Classification (k =\", k_value, \")\")) +\n  scale_color_discrete(name = \"Actual Class\") +\n  scale_shape_discrete(name = \"Predicted Class\") +\n  theme_minimal()\n\n\n\n\nPrint the confusion matrix\n\nconfusion_matrix &lt;- table(test.Y, pred.Y)\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(confusion_matrix)\n\n            pred.Y\ntest.Y       setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          3        20\n\n\nCalculate the misclassification rate\n\nprint(paste(\"Misclassification Rate:\", mcr))\n\n[1] \"Misclassification Rate: 0.0533333333333333\"\n\n\n\nprint(paste(\"Misclassification Rate:\", mcr))\n\n[1] \"Misclassification Rate: 0.0533333333333333\""
  },
  {
    "objectID": "posts/Probability/index.html#introduction",
    "href": "posts/Probability/index.html#introduction",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Load necessary libraries\n\nlibrary(class)\nlibrary(ggplot2)\n\nSet the random seed for reproducibility\n\nset.seed(0)\n\nSplit data into training and test sets\n\nnsample = nrow(iris)\ntrain.ix = sample(1:nsample, round(nsample / 2))\ntest.ix = setdiff(1:nsample, train.ix)\ntrain.X = iris[train.ix, 3:4]  # Only use Petal.Length, Petal.Width\ntrain.Y = iris[train.ix, 5]    # Factor data type\ntest.X = iris[test.ix, 3:4]\ntest.Y = iris[test.ix, 5]\n\nPerform classification for the test set\n\nk_value = 3\npred.Y = knn(train.X, test.X, train.Y, k = k_value)\nmcr = mean(test.Y != pred.Y)\n\nCreate a data frame to store test data and predicted labels\n\nresults_df &lt;- data.frame(\n  Petal.Length = test.X[, 1],\n  Petal.Width = test.X[, 2],\n  Actual_Class = test.Y,\n  Predicted_Class = pred.Y\n)\n\nPlot the test data points color-coded by their predicted class\n\nggplot(results_df, aes(x = Petal.Length, y = Petal.Width, color = factor(Actual_Class), shape = factor(Predicted_Class))) +\n  geom_point(size = 3) +\n  labs(x = \"Petal Length\", y = \"Petal Width\", title = paste(\"KNN Classification (k =\", k_value, \")\")) +\n  scale_color_discrete(name = \"Actual Class\") +\n  scale_shape_discrete(name = \"Predicted Class\") +\n  theme_minimal()\n\n\n\n\nPrint the confusion matrix\n\nconfusion_matrix &lt;- table(test.Y, pred.Y)\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(confusion_matrix)\n\n            pred.Y\ntest.Y       setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          3        20\n\n\nCalculate the misclassification rate\n\nprint(paste(\"Misclassification Rate:\", mcr))\n\n[1] \"Misclassification Rate: 0.0533333333333333\"\n\n\n\nprint(paste(\"Misclassification Rate:\", mcr))\n\n[1] \"Misclassification Rate: 0.0533333333333333\""
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html",
    "href": "posts/Classification_Crop suitability/index.html",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html#introduction",
    "href": "posts/Classification_Crop suitability/index.html#introduction",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html#random-forest-rf",
    "href": "posts/Classification_Crop suitability/index.html#random-forest-rf",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Random Forest (RF)",
    "text": "Random Forest (RF)\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees."
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html#support-vector-machines-svm",
    "href": "posts/Classification_Crop suitability/index.html#support-vector-machines-svm",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It’s effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html#naive-bayes-nb",
    "href": "posts/Classification_Crop suitability/index.html#naive-bayes-nb",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Naive Bayes (NB)",
    "text": "Naive Bayes (NB)\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area’s average temperature, humidity, pH level, rainfall, and NPK values."
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html#the-data",
    "href": "posts/Classification_Crop suitability/index.html#the-data",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "The Data",
    "text": "The Data\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type.\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R’s head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R’s is.na() function."
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html#data-visualization",
    "href": "posts/Classification_Crop suitability/index.html#data-visualization",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo get a better understanding of the data, I visualized it using R’s ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns."
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html#data-processing",
    "href": "posts/Classification_Crop suitability/index.html#data-processing",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Processing",
    "text": "Data Processing\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label."
  },
  {
    "objectID": "posts/Classification_Crop suitability/index.html#model-selection",
    "href": "posts/Classification_Crop suitability/index.html#model-selection",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Model Selection",
    "text": "Model Selection\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\nLoad necessary libraries:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pheatmap)\n\nLoad the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kchhetri.github.io/Crop_recommendation.csv\")\n\n\nset.seed(123)\n\nSplit the data into training and testing sets:\n\n# See the head and tail of data\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n\nCreate a data frame to store test data and predicted labels:\n\n# See the distribution of data \nsummary(data)\n\n       N                P                K           temperature    \n Min.   :  0.00   Min.   :  5.00   Min.   :  5.00   Min.   : 8.826  \n 1st Qu.: 21.00   1st Qu.: 28.00   1st Qu.: 20.00   1st Qu.:22.769  \n Median : 37.00   Median : 51.00   Median : 32.00   Median :25.599  \n Mean   : 50.55   Mean   : 53.36   Mean   : 48.15   Mean   :25.616  \n 3rd Qu.: 84.25   3rd Qu.: 68.00   3rd Qu.: 49.00   3rd Qu.:28.562  \n Max.   :140.00   Max.   :145.00   Max.   :205.00   Max.   :43.675  \n    humidity           ph           rainfall         label          \n Min.   :14.26   Min.   :3.505   Min.   : 20.21   Length:2200       \n 1st Qu.:60.26   1st Qu.:5.972   1st Qu.: 64.55   Class :character  \n Median :80.47   Median :6.425   Median : 94.87   Mode  :character  \n Mean   :71.48   Mean   :6.469   Mean   :103.46                     \n 3rd Qu.:89.95   3rd Qu.:6.924   3rd Qu.:124.27                     \n Max.   :99.98   Max.   :9.935   Max.   :298.56                     \n\n\nSee the relationship between different parameters such as Ph vs label or precipitation vs crop:\n\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n\n\n\n\n\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n\n\n\n\nData processing: separate feature and target variable:\n\nfeatures &lt;- data[,1:7]\ntarget &lt;- data$label\n\nSplit the data into training and testing sets:\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(target, p=0.8, list=FALSE)\ntrainData &lt;- features[trainIndex, ]\ntrainLabels &lt;- target[trainIndex]\ntestData &lt;- features[-trainIndex, ]\ntestLabels &lt;- target[-trainIndex]\n\nDefine training control:\n\ntrain_control &lt;- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm &lt;- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb &lt;- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels &lt;- as.factor(trainLabels)\ntestLabels &lt;- as.factor(testLabels)\n\nMake predictions on the test data:\nUsing Random Forest:\n\npred_rf &lt;- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf &lt;- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric &lt;- as.matrix(cm_rf)\ncm_rf_numeric[] &lt;- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n\n\n\n\nFor SVM\n\npred_svm &lt;- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm &lt;- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\nFor NB\n\npred_nb &lt;- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb &lt;- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric &lt;- as.matrix(cm_nb)\ncm_nb_numeric[] &lt;- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n\n\nCalculate and print misclassification rates:\n\nmis_rf &lt;- 1 - cm_rf$overall['Accuracy']\nmis_svm &lt;- 1 - cm_svm$overall['Accuracy']\nmis_nb &lt;- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification &lt;- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n          Model Misclassification_Rate\n1 Random Forest            0.004545455\n2           SVM            0.018181818\n3   Naive Bayes            0.004545455\n\n\n\n# Summarize the results\nresults &lt;- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: RF, SVM, NB \nNumber of resamples: 10 \n\nAccuracy \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9886364 0.9943182 0.9943182 0.9954545 0.9985795    1    0\nSVM 0.9659091 0.9730114 0.9829545 0.9829545 0.9928977    1    0\nNB  0.9772727 0.9943182 0.9943182 0.9943182 1.0000000    1    0\n\nKappa \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9880952 0.9940476 0.9940476 0.9952381 0.9985119    1    0\nSVM 0.9642857 0.9717262 0.9821429 0.9821429 0.9925595    1    0\nNB  0.9761905 0.9940476 0.9940476 0.9940476 1.0000000    1    0\n\n\n\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n\n\n\n\nMaking Predictions:\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n# Train the model\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData &lt;- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction &lt;- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\n\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\nConclusion\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant."
>>>>>>> 59d757e7d394f5f4f5d9f096b3d37f9fbb9321dd
  }
]