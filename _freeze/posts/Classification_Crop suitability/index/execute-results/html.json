{
  "hash": "83b0b20d72ee58632aa69bcd66d68a9a",
  "result": {
    "markdown": "---\ntitle: \"Predicting Crop Suitability with Machine Learning using R \"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-28\"\ncategories:\n  - R\n  - Code\n  - Analysis\n---\n\n\n## Introduction\n\nMachine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring.\n\n## Random Forest (RF)\n\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees.\n\n## Support Vector Machines (SVM)\n\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It's effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\n## Naive Bayes (NB)\n\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Na√Øve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\n\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area's average temperature, humidity, pH level, rainfall, and NPK values.\n\n## The Data\n\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type.\n\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R's head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R's is.na() function.\n\n## Data Visualization\n\nTo get a better understanding of the data, I visualized it using R's ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns.\n\n## Data Processing\n\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label.\n\n## Model Selection\n\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\n\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\n\n[**Load necessary libraries:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n```{.r .cell-code}\nlibrary(pheatmap)\n```\n:::\n\n\n[**Load the data:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/Crop_recommendation.csv\")\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n\n[**Split the data into training and testing sets:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See the head and tail of data\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for missing values\nsum(is.na(data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n[**Create a data frame to store test data and predicted labels:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See the distribution of data \nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       N                P                K           temperature    \n Min.   :  0.00   Min.   :  5.00   Min.   :  5.00   Min.   : 8.826  \n 1st Qu.: 21.00   1st Qu.: 28.00   1st Qu.: 20.00   1st Qu.:22.769  \n Median : 37.00   Median : 51.00   Median : 32.00   Median :25.599  \n Mean   : 50.55   Mean   : 53.36   Mean   : 48.15   Mean   :25.616  \n 3rd Qu.: 84.25   3rd Qu.: 68.00   3rd Qu.: 49.00   3rd Qu.:28.562  \n Max.   :140.00   Max.   :145.00   Max.   :205.00   Max.   :43.675  \n    humidity           ph           rainfall         label          \n Min.   :14.26   Min.   :3.505   Min.   : 20.21   Length:2200       \n 1st Qu.:60.26   1st Qu.:5.972   1st Qu.: 64.55   Class :character  \n Median :80.47   Median :6.425   Median : 94.87   Mode  :character  \n Mean   :71.48   Mean   :6.469   Mean   :103.46                     \n 3rd Qu.:89.95   3rd Qu.:6.924   3rd Qu.:124.27                     \n Max.   :99.98   Max.   :9.935   Max.   :298.56                     \n```\n:::\n:::\n\n\n[**See the relationship between different parameters such as Ph vs label or precipitation vs crop:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n[**Data processing: separate feature and target variable:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeatures <- data[,1:7]\ntarget <- data$label\n```\n:::\n\n\n[**Split the data into training and testing sets:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntrainIndex <- createDataPartition(target, p=0.8, list=FALSE)\ntrainData <- features[trainIndex, ]\ntrainLabels <- target[trainIndex]\ntestData <- features[-trainIndex, ]\ntestLabels <- target[-trainIndex]\n```\n:::\n\n\n[**Define training control:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_control <- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf <- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm <- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb <- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels <- as.factor(trainLabels)\ntestLabels <- as.factor(testLabels)\n```\n:::\n\n\n[**Make predictions on the test data:**]{.underline}\n\n**Using Random Forest:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_rf <- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf <- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric <- as.matrix(cm_rf)\ncm_rf_numeric[] <- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n**For SVM**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_svm <- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm <- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n```\n:::\n\n\n**For NB**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_nb <- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb <- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric <- as.matrix(cm_nb)\ncm_nb_numeric[] <- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n[**Calculate and print misclassification rates:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmis_rf <- 1 - cm_rf$overall['Accuracy']\nmis_svm <- 1 - cm_svm$overall['Accuracy']\nmis_nb <- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification <- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Model Misclassification_Rate\n1 Random Forest            0.004545455\n2           SVM            0.018181818\n3   Naive Bayes            0.004545455\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summarize the results\nresults <- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsummary.resamples(object = results)\n\nModels: RF, SVM, NB \nNumber of resamples: 10 \n\nAccuracy \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9886364 0.9943182 0.9943182 0.9954545 0.9985795    1    0\nSVM 0.9659091 0.9730114 0.9829545 0.9829545 0.9928977    1    0\nNB  0.9772727 0.9943182 0.9943182 0.9943182 1.0000000    1    0\n\nKappa \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9880952 0.9940476 0.9940476 0.9952381 0.9985119    1    0\nSVM 0.9642857 0.9717262 0.9821429 0.9821429 0.9925595    1    0\nNB  0.9761905 0.9940476 0.9940476 0.9940476 1.0000000    1    0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n[**Making Predictions:**]{.underline}\n\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the model\nmodel_rf <- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData <- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction <- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The suitable crop for Blackstone Area is: maize\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The suitable crop for Blackstone Area is: maize\"\n```\n:::\n:::\n\n\nConclusion\n\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}