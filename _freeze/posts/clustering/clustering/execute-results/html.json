{
  "hash": "1d9aaba6cc0f51a758c65d88de7f185a",
  "result": {
    "markdown": "---\ntitle: \"DNSCAN Clustering\"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-18\"\ncategories:\n  - News\n  - Code\n  - Analysis\n---\n\n\n# Understanding DBSCAN Clustering Analysis in Machine Learning\n\n## Introduction\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data\\'s density.\n\n## How DBSCAN Works\n\nDBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least **`minPts`** within a radius of **`eps`** from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset.\n\n## Advantages of DBSCAN\n\nDBSCAN has several advantages over other clustering algorithms:\n\n1.  **No need to specify the number of clusters**: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\n\n2.  **Ability to find arbitrarily shaped clusters**: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\n\n3.  **Robustness to noise**: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria.\n\n## Disadvantages of DBSCAN\n\nDespite its advantages, DBSCAN also has some limitations:\n\n1.  **Difficulty handling varying densities**: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single **`eps`** and **`minPts`** value may not be suitable for all clusters.\n\n2.  **Sensitivity to parameter settings**: The results of DBSCAN can be significantly affected by the settings of **`eps`** and **`minPts`**. Choosing appropriate values for these parameters can be challenging.\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\n\nLoad necessary libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fpc) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the data into R\ndata <- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n```\n:::\n:::\n\nThe dataset contains information about individuals such as their age, sex, BMI, number of children, smoking status, region, and charges.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ...\n```\n:::\n:::\n\n\n# Preprocessing the Data\nBefore we can perform DBSCAN clustering, we need to preprocess the data. This typically involves normalizing the data and converting categorical variables into numerical variables. However, for simplicity, let’s just use the numerical columns in our dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_num <- data[, sapply(data, is.numeric)]\nhead(data_num)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  age    bmi children   charges\n1  19 27.900        0 16884.924\n2  18 33.770        1  1725.552\n3  28 33.000        3  4449.462\n4  33 22.705        0 21984.471\n5  32 28.880        0  3866.855\n6  31 25.740        0  3756.622\n```\n:::\n:::\n\n\n\n\n# Perform DBSCAN on the loaded dataset:\n## Remove label form dataset\n\n::: {.cell}\n\n```{.r .cell-code}\nmed <- data_num[-4] \n```\n:::\n\n\nFitting DBSCAN clustering model:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0)  # Setting seed \nDbscan_cl <- dbscan(med, eps = 0.45, MinPts = 5) \nDbscan_cl\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndbscan Pts=1338 MinPts=5 eps=0.45\n          0 1 2 3 4 5\nborder 1310 5 2 4 3 4\nseed      0 1 5 1 1 2\ntotal  1310 6 7 5 4 6\n```\n:::\n:::\n\nIn this code, eps is the maximum distance between two samples for them to be considered as in the same neighborhood, and MinPts is the number of samples in a neighborhood for a point to be considered as a core point.\n\n# Visualizing the Results\nFinally, let’s visualize the results. We will create a scatter plot of the data, with each point colored according to its cluster assignment:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Table \ntable(Dbscan_cl$cluster, data_num$age) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n    18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\n  0 63 46 29 28 28 28 28 28 28 28 28 27 27 27 26 26 26 25 25 25 25 25 27 27 27\n  1  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  2  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  3  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  4  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  5  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   \n    43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\n  0 27 27 29 29 29 29 28 29 29 29 28 28 26 26 26 25 25 23 23 23 23 22\n  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plotting Cluster \nplot(Dbscan_cl, med, main = \"DBScan\") \n```\n\n::: {.cell-output-display}\n![](clustering_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nIn this plot, points that belong to the same cluster have the same color.\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}