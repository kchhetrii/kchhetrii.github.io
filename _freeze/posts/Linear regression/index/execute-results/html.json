{
<<<<<<< HEAD
  "hash": "3a13a28f5098b82546032545a5bbdf68",
  "result": {
    "markdown": "---\ntitle: \"Linear Regression\"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-18\"\ncategories:\n  - R\n  - Code\n  - Analysis\n---\n\n\n### Introduction\n\nLinear regression is a powerful statistical method that allows us to examine the relationship between two (simple linear regression) or more (multiple linear regression) variables. A key feature of linear regression is its simplicity and interpretability. It's used in various fields, including machine learning, most medical fields, and social sciences.\n\nLinear regression models the relationship between two variables by fitting a linear equation to observed data. The steps to perform multiple linear regression are almost identical to those of simple linear regression.\n\n![](https://editor.analyticsvidhya.com/uploads/375512.jpg)\n\n[**Types of regression:**]{.underline}\n\na\\. Simple linear regression: In this case, we use only one input variable\n\nb\\. Multiple linear regression: In this case, We use multiple input variable\n\n### The Dataset\n\nI will update about it soon.\n\n### Assumptions:\n\nThere are several assumption for the linear regression model that I am using in this blog. They are explained below, lets take a look.\n\n[**Linearity:**]{.underline} The relationship between the independent and dependent variables is linear. This assumption can be checked by plotting the variables and examining the data scatter.\n\n[**Independence:**]{.underline} The residuals (the differences between the observed and predicted values) are independent. In other words, there is not a specific pattern in the residuals.\n\n[**Homoscedasticity:**]{.underline} The variance of the residuals is constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all predicted values.\n\n[**Normality:**]{.underline} The residuals are normally distributed. If this assumption is violated, then the confidence intervals may not be accurate.\n\n[**Absence of multicollinearity:**]{.underline} In the case of multiple linear regression, the independent variables should not be too highly correlated with each other.\n\n[**Load necessary libraries:**]{.underline}\n\n\n::: {.cell}\n\n:::\n\n\n[**Set the random seed for reproducibility:**]{.underline}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ncorrplot 0.92 loaded\n```\n:::\n\n```{.r .cell-code}\nlibrary(reshape2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the data into R\ndata <- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n```\n:::\n:::\n\n\n[**Load and clean the data:**]{.underline}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for missing values\nsum(is.na(data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# Handle missing values (if any)\n# data <- na.omit(data)  # drops rows with missing values\n# data <- data[complete.cases(data), ]  # drops rows with missing values\n# data$column <- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nSince, we don't have any null or missing value in this data as indicated above. So, no further handling of missing data is needed.\n\n\n::: {.cell}\n\n:::\n\n\nVisualize the data: for better understanding of the data before proceeding further\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLets take a look for the correlation between these two features: Our data contains one column \"Species\" which is non-numeric. Thus, we will firstly subset the data that doesnot contains \"Species\" column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Subset the data to include only numeric columns\ndata_numeric <- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix <- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n```\n:::\n:::\n\n\nCreate the heatmap for better visualization:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Melt the correlation matrix into a long format\ndata_melt <- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nFrom the heatmap created using the above code, we can see there is no strong correlation between different variables. However, this part is made to visualize the data only.\n\n[**Building the model:**]{.underline} From the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a violin plot for 'age'\nggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellow\") +\n  labs(title=\"Distribution of Charges by Sex\", x=\"Sex\", y=\"Charges\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Create a violin plot for 'charges' vs 'sex'\np1 <- ggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellowgreen\") +\n  labs(title=\"Violin plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a violin plot for 'charges' vs 'smoker'\np2 <- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_violin(fill=\"goldenrod1\") +\n  labs(title=\"Violin plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(p2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n:::\n\n\nAccording to the left plot, the average insurance cost for men and women is roughly the same at \\$5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about \\$5,000. The minimum insurance premium for smokers is \\$5,000.\n\n#### Need to check again on these\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the data into training and test sets\nset.seed(0)\nsampleSplit <- sample.split(Y=data$bmi, SplitRatio=0.7)\ntrainSet <- subset(x=data, sampleSplit==TRUE)\ntestSet <- subset(x=data, sampleSplit==FALSE)\n\n\n# Fit the model\nmodel <- lm(formula=bmi ~ ., data=trainSet)\n```\n:::\n\n\nBy using a linear combination of all other attributes, we hope to predict the bmi attribute. In this case, we don't have to worry about the categorical attributes. R automatically manages the categorical attributes.\n\nLets see the summary of model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the data into training and test sets\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = bmi ~ ., data = trainSet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.9234  -3.7146   0.0124   3.4208  20.9629 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      2.766e+01  6.883e-01  40.181  < 2e-16 ***\nage             -2.573e-02  1.521e-02  -1.692   0.0910 .  \nsexmale          7.230e-01  3.679e-01   1.965   0.0497 *  \nchildren        -2.269e-02  1.504e-01  -0.151   0.8801    \nsmokeryes       -6.980e+00  7.894e-01  -8.841  < 2e-16 ***\nregionnorthwest -2.051e-01  5.294e-01  -0.387   0.6986    \nregionsoutheast  3.923e+00  5.169e-01   7.589 7.58e-14 ***\nregionsouthwest  1.379e+00  5.307e-01   2.599   0.0095 ** \ncharges          2.803e-04  2.771e-05  10.119  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.73 on 967 degrees of freedom\nMultiple R-squared:  0.181,\tAdjusted R-squared:  0.1742 \nF-statistic: 26.71 on 8 and 967 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe P-values that are shown in the Pr(\\>\\|t\\|) column are the most intriguing aspect of this situation. The probability significance column Pr(\\>\\|t\\|) in the above table is a crucial factor to take into account because it shows how important probability is for prediction. Here, we took a significance level of 5%  for the calculation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelResiduals <- as.data.frame(residuals(model))\nggplot(modelResiduals, aes(residuals(model)))+\n  geom_histogram(fill = 'mediumorchid1', color = 'black')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThe distribution of the residual data is approximately normally distributed.\n\nMake the prediction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- predict(model, testSet)\n```\n:::\n\n\nNow, see the further details by creating a table for for actual and prediction values from the above created model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelEval <- cbind(testSet$bmi, preds)\ncolnames(modelEval) <- c('Acual', 'Predicted')\nmodelEval <- as.data.frame(modelEval)\nhead(modelEval)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Acual Predicted\n4  22.705  33.49025\n14 39.820  33.24960\n16 24.600  29.76384\n23 34.100  32.15970\n24 31.920  30.35069\n27 23.085  30.08882\n```\n:::\n:::\n\n\n# **Now lets evaluate the prediction:**\n\n[**Model evaluation:**]{.underline} After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse).\n\n[**Lets learn little bit about rmse:**]{.underline} The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\n\nDuring model evaluation, RMSE serves as a measure to understand the model's performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate error metrics\nmse <- mean((modelEval$Acual - modelEval$Predicted)^2)\nrmse <- sqrt(mse)\nprint(rmse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.962662\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
=======
  "hash": "568ae4458eed056d7deae8186c684b1d",
  "result": {
    "markdown": "---\ntitle: \"K-Nearest Neighbors (KNN) with Iris Dataset in R\"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-18\"\ncategories:\n  - News\n  - Code\n  - Analysis\n---\n\n\n## Introduction\n\nK-Nearest Neighbors (KNN) is a simple yet powerful classification algorithm. It classifies a data point based on how its neighbors are classified. In this blog post, we will use the Iris dataset to understand how KNN works using R.\n\n## What is KNN?\n\nKNN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. It's both a classification and regression method.\n\n## The Iris Dataset\n\nThe Iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The dataset consists of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the lengths and the widths of the sepals and petals.\n\nLoad necessary libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(class)\nlibrary(ggplot2)\n```\n:::\n\n\nSet the random seed for reproducibility\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlst = [1,2,3]\nlst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1, 2, 3]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0)\n```\n:::\n\n\nSplit data into training and test sets\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsample = nrow(iris)\ntrain.ix = sample(1:nsample, round(nsample / 2))\ntest.ix = setdiff(1:nsample, train.ix)\ntrain.X = iris[train.ix, 3:4]  # Only use Petal.Length, Petal.Width\ntrain.Y = iris[train.ix, 5]    # Factor data type\ntest.X = iris[test.ix, 3:4]\ntest.Y = iris[test.ix, 5]\n```\n:::\n\n\nPerform classification for the test set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk_value = 3\npred.Y = knn(train.X, test.X, train.Y, k = k_value)\nmcr = mean(test.Y != pred.Y)\n```\n:::\n\n\nCreate a data frame to store test data and predicted labels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_df <- data.frame(\n  Petal.Length = test.X[, 1],\n  Petal.Width = test.X[, 2],\n  Actual_Class = test.Y,\n  Predicted_Class = pred.Y\n)\n```\n:::\n\n\nPlot the test data points color-coded by their predicted class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(results_df, aes(x = Petal.Length, y = Petal.Width, color = factor(Actual_Class), shape = factor(Predicted_Class))) +\n  geom_point(size = 3) +\n  labs(x = \"Petal Length\", y = \"Petal Width\", title = paste(\"KNN Classification (k =\", k_value, \")\")) +\n  scale_color_discrete(name = \"Actual Class\") +\n  scale_shape_discrete(name = \"Predicted Class\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nPrint the confusion matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusion_matrix <- table(test.Y, pred.Y)\nprint(\"Confusion Matrix:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Confusion Matrix:\"\n```\n:::\n\n```{.r .cell-code}\nprint(confusion_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            pred.Y\ntest.Y       setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          3        20\n```\n:::\n:::\n\n\nCalculate the misclassification rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste(\"Misclassification Rate:\", mcr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Misclassification Rate: 0.0533333333333333\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste(\"Misclassification Rate:\", mcr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Misclassification Rate: 0.0533333333333333\"\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n",
>>>>>>> 59d757e7d394f5f4f5d9f096b3d37f9fbb9321dd
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}