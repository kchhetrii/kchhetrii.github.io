{
  "hash": "568ae4458eed056d7deae8186c684b1d",
  "result": {
    "markdown": "---\ntitle: \"K-Nearest Neighbors (KNN) with Iris Dataset in R\"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-18\"\ncategories:\n  - News\n  - Code\n  - Analysis\n---\n\n\n## Introduction\n\nK-Nearest Neighbors (KNN) is a simple yet powerful classification algorithm. It classifies a data point based on how its neighbors are classified. In this blog post, we will use the Iris dataset to understand how KNN works using R.\n\n## What is KNN?\n\nKNN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. It's both a classification and regression method.\n\n## The Iris Dataset\n\nThe Iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The dataset consists of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the lengths and the widths of the sepals and petals.\n\nLoad necessary libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(class)\nlibrary(ggplot2)\n```\n:::\n\n\nSet the random seed for reproducibility\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlst = [1,2,3]\nlst\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1, 2, 3]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0)\n```\n:::\n\n\nSplit data into training and test sets\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsample = nrow(iris)\ntrain.ix = sample(1:nsample, round(nsample / 2))\ntest.ix = setdiff(1:nsample, train.ix)\ntrain.X = iris[train.ix, 3:4]  # Only use Petal.Length, Petal.Width\ntrain.Y = iris[train.ix, 5]    # Factor data type\ntest.X = iris[test.ix, 3:4]\ntest.Y = iris[test.ix, 5]\n```\n:::\n\n\nPerform classification for the test set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk_value = 3\npred.Y = knn(train.X, test.X, train.Y, k = k_value)\nmcr = mean(test.Y != pred.Y)\n```\n:::\n\n\nCreate a data frame to store test data and predicted labels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_df <- data.frame(\n  Petal.Length = test.X[, 1],\n  Petal.Width = test.X[, 2],\n  Actual_Class = test.Y,\n  Predicted_Class = pred.Y\n)\n```\n:::\n\n\nPlot the test data points color-coded by their predicted class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(results_df, aes(x = Petal.Length, y = Petal.Width, color = factor(Actual_Class), shape = factor(Predicted_Class))) +\n  geom_point(size = 3) +\n  labs(x = \"Petal Length\", y = \"Petal Width\", title = paste(\"KNN Classification (k =\", k_value, \")\")) +\n  scale_color_discrete(name = \"Actual Class\") +\n  scale_shape_discrete(name = \"Predicted Class\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nPrint the confusion matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusion_matrix <- table(test.Y, pred.Y)\nprint(\"Confusion Matrix:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Confusion Matrix:\"\n```\n:::\n\n```{.r .cell-code}\nprint(confusion_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            pred.Y\ntest.Y       setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          3        20\n```\n:::\n:::\n\n\nCalculate the misclassification rate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste(\"Misclassification Rate:\", mcr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Misclassification Rate: 0.0533333333333333\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(paste(\"Misclassification Rate:\", mcr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Misclassification Rate: 0.0533333333333333\"\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}