{
  "hash": "9973cc247e32da1cbf6c14fe4aacf22d",
  "result": {
    "markdown": "---\ntitle: \"Decision tree and random forest with R\"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-25\"\ncategories:\n  - News\n  - Code\n  - Analysis\n---\n\n\n## **Introduction:** \n\nDecision trees are a popular machine learning algorithm due to their simplicity, interpretability, and versatility. They can be used for both regression and classification problems. In this blog post, we will walk you through how to implement a decision tree in R.\n\nDecision Trees are a type of supervised Machine Learning algorithm used for both regression and classification problems. They are tree-based models that split the data into smaller subsets based on certain conditions. The final output is obtained by combining the results of multiple splits.\n\n**The Iris Dataset:**\n\nThe Iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. The dataset consists of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the lengths and the widths of the sepals and petals.\n\nLoad necessary libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\n```\n:::\n\n\n### Step 1: Import the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsample = nrow(iris)\n```\n:::\n\n\nSet the random seed for reproducibility\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0)\n```\n:::\n\n\n### Step 2: Split data into training and test sets\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split data into training and test sets\ntrain.ix = sample(1 : nsample, round(nsample / 2))\ntest.ix = setdiff(1 : nsample, train.ix)\ntrain.df = iris[train.ix, ]; test.df = iris[test.ix, ]\n```\n:::\n\n\n### Step 3: Build the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctmodel = rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length\n                + Petal.Width, method = \"class\", data = train.df)\nsummary(ctmodel) # detailed summary of splits\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nrpart(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + \n    Petal.Width, data = train.df, method = \"class\")\n  n= 75 \n\n         CP nsplit  rel error     xerror       xstd\n1 0.5869565      0 1.00000000 1.08695652 0.08874963\n2 0.3913043      1 0.41304348 0.41304348 0.08188085\n3 0.0100000      2 0.02173913 0.02173913 0.02159372\n\nVariable importance\n Petal.Width Petal.Length Sepal.Length  Sepal.Width \n          33           30           22           15 \n\nNode number 1: 75 observations,    complexity param=0.5869565\n  predicted class=setosa      expected loss=0.6133333  P(node) =1\n    class counts:    29    19    27\n   probabilities: 0.387 0.253 0.360 \n  left son=2 (29 obs) right son=3 (46 obs)\n  Primary splits:\n      Petal.Length < 2.45 to the left,  improve=26.94899, (0 missing)\n      Petal.Width  < 0.8  to the left,  improve=26.94899, (0 missing)\n      Sepal.Length < 5.55 to the left,  improve=22.28490, (0 missing)\n      Sepal.Width  < 3.35 to the right, improve=11.74540, (0 missing)\n  Surrogate splits:\n      Petal.Width  < 0.8  to the left,  agree=1.000, adj=1.000, (0 split)\n      Sepal.Length < 5.45 to the left,  agree=0.933, adj=0.828, (0 split)\n      Sepal.Width  < 3.35 to the right, agree=0.840, adj=0.586, (0 split)\n\nNode number 2: 29 observations\n  predicted class=setosa      expected loss=0  P(node) =0.3866667\n    class counts:    29     0     0\n   probabilities: 1.000 0.000 0.000 \n\nNode number 3: 46 observations,    complexity param=0.3913043\n  predicted class=virginica   expected loss=0.4130435  P(node) =0.6133333\n    class counts:     0    19    27\n   probabilities: 0.000 0.413 0.587 \n  left son=6 (20 obs) right son=7 (26 obs)\n  Primary splits:\n      Petal.Width  < 1.75 to the left,  improve=20.404350, (0 missing)\n      Petal.Length < 4.75 to the left,  improve=16.904350, (0 missing)\n      Sepal.Length < 6.15 to the left,  improve= 6.787779, (0 missing)\n      Sepal.Width  < 2.75 to the left,  improve= 3.686701, (0 missing)\n  Surrogate splits:\n      Petal.Length < 4.75 to the left,  agree=0.913, adj=0.80, (0 split)\n      Sepal.Length < 6.15 to the left,  agree=0.761, adj=0.45, (0 split)\n      Sepal.Width  < 2.75 to the left,  agree=0.696, adj=0.30, (0 split)\n\nNode number 6: 20 observations\n  predicted class=versicolor  expected loss=0.05  P(node) =0.2666667\n    class counts:     0    19     1\n   probabilities: 0.000 0.950 0.050 \n\nNode number 7: 26 observations\n  predicted class=virginica   expected loss=0  P(node) =0.3466667\n    class counts:     0     0    26\n   probabilities: 0.000 0.000 1.000 \n```\n:::\n:::\n\n\n**Visualize the decision tree:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(rpart.plot)\nlibrary(ggplot2)\n\n# Plot the decision tree\nrpart.plot(ctmodel, type = 4, extra = 2, tweak = 1.3, varlen = 0, nn = TRUE, fallen.leaves = TRUE,\n           box.palette = \"GnBu\", shadow.col = \"gray\", main = \"Classification Tree for Iris Data\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n**Perform classification for test set:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred.id = predict(ctmodel, test.df, type = \"class\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# pred.mat = predict(ctmodel, test.df)\n# pred.numid = apply(pred.mat, 1, which.max)\n# mcr = mean(pred.numid != as.numeric(test.df$Species))\n# pred.id = cut(pred.numid, breaks = 0 : 3 + 0.5, labels = levels(test.id))\ntest.id = test.df$Species\nmcr = mean(pred.id != test.id)\ntable(test.id, pred.id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            pred.id\ntest.id      setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          4        19\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n# Random forest\n\n[**Introduction to Random Forests:**]{.underline}\n\nRandom Forest is a powerful Machine Learning algorithm that was first introduced by Leo Breiman and Adele Cutler in 2001. It is an ensemble learning method that operates by constructing multiple decision trees during training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\n[**Advantages of Random Forests:**]{.underline}\n\n**Excellent Predictive Powers:** Random Forests have excellent predictive capabilities, making them useful for applications where accuracy really matters.\n\n**Robust to Outliers:** Random Forests are robust to outliers.\n\n**Works Well with Non-linear Data:** Random Forests work well with non-linear data.\n\n**Lower Risk of Overfitting:** Compared to decision trees, Random Forests have a lower risk of overfitting.\n\n**Efficient with Large Datasets:** Random Forests run efficiently on large datasets.\n\n**Handles Missing Data Well:** Just like decision trees, random forests handle missing values effectively.\n\n[**Disadvantages of Random Forests:**]{.underline}\n\n**Requires Much Computational Power and Resources:** As it builds numerous trees to combine their outputs, it requires much computational power and resources.\n\n**Requires Much Time for Training:** It requires much time for training as it combines a lot of decision trees to determine the class.\n\n**Interpretability Issues:** Due to the ensemble of decision trees, it also suffers interpretability and fails to determine the significance of each variable.\n\n**Overfitting Risk:** Although much lower than decision trees, overfitting is still a risk with random forests and something you should monitor.\n\n**Limited with Regression:** While random forest is almost unmatched in most classification solutions, it can be limited with regression, especially when data has a linear nature.\n\nLets understand this from the real example:\n\n**Load the necessary libarary:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrfmodel = randomForest(Species ~ Sepal.Length + Sepal.Width\n                       + Petal.Length + Petal.Width, data = train.df)\nprint(rfmodel) # view results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(formula = Species ~ Sepal.Length + Sepal.Width +      Petal.Length + Petal.Width, data = train.df) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 2.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         29          0         0  0.00000000\nversicolor      0         18         1  0.05263158\nvirginica       0          1        26  0.03703704\n```\n:::\n\n```{.r .cell-code}\nimportance(rfmodel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             MeanDecreaseGini\nSepal.Length        5.3646953\nSepal.Width         0.8873669\nPetal.Length       18.6814751\nPetal.Width        23.6697027\n```\n:::\n:::\n\n\n**Perform classification for test set:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred.id = predict(rfmodel, test.df)\ntest.id = test.df$Species\n```\n:::\n\n\n**Misclassification calculation rate:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcr = mean(pred.id != test.id)\ntable(test.id, pred.id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            pred.id\ntest.id      setosa versicolor virginica\n  setosa         21          0         0\n  versicolor      0         30         1\n  virginica       0          4        19\n```\n:::\n\n```{.r .cell-code}\nctmodel.2 = getTree(rfmodel, k = 2) # get the second tree\nprint(ctmodel.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  left daughter right daughter split var split point status prediction\n1             2              3         3        2.85      1          0\n2             0              0         0        0.00     -1          1\n3             4              5         4        1.75      1          0\n4             0              0         0        0.00     -1          2\n5             0              0         0        0.00     -1          3\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}