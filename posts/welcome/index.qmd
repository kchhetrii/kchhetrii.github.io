---
title: "Introduction to Linear Regression using Machine Learning"
author: "Kamal Chhetri"
date: "2023-10-18"
categories:
  - News
  - Code
  - Analysis
---

# Linear Regression:

Linear Regression is one of the most fundamental and widely known Machine Learning Algorithms which people start with. Building blocks of a Linear Regression Model are:

-   **Dependent Variable**: The main factor in Linear Regression. It is the variable that we want to predict or forecast.

-   **Independent Variables**: These variables are the factors that we hypothesize have an impact on our dependent variable.

## What is Linear Regression?

Linear Regression is based on the equation of a straight line, **`y = mx + c`**, where **`y`** is the dependent variable we want to predict, **`x`** is the independent variable we use to make the prediction, **`m`** is the slope of the line, and **`c`** is the y-intercept.

In a machine learning context, **`y`** is often called the target or label, and **`x`** is called a feature. The terms **`m`** and **`c`** are parameters of the model that are learned from the training data.

## Why Use Linear Regression?

Linear regression is easy to understand and explain, making it a popular tool for business analyses. It works best when predictors are independent of each other, and when they have a linear relationship to the dependent variable.

![](https://miro.medium.com/v2/1*GBjasHYN0y3cKO-edveV4w.png)

## Limitations of Linear Regression

While linear regression is straightforward and easy to understand, it does have some limitations:

-   It assumes a linear relationship between input variables and output. If this relationship isn't linear, linear regression may not provide a good fit to the data.

-   It's sensitive to outliers. A few extreme data points can significantly skew your regression line.

-   It assumes that all variables are independent of each other. If variables are correlated (known as multicollinearity), it can affect performance.
